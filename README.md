# LLM-in-a-box

Host a local LLM accessible on any device on your local network, with 100% local models and Ollama

## Installation

TODO: Ollama setup

1. Clone the repo

```bash
   git clone <repository-url>
   cd my_flask_app
```

2. Create a virtual environment (optional but recommended)

```bash
python3 -m venv venv
source venv/bin/activate  # On Windows use `venv\Scripts\activate`
```

3.Install dependencies

```bash
pip3 install -r requirements.txt
```

4. Run

```bash
python app.py

```

## Interface

TODO
